
\documentclass[french]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{babel}
\usepackage{blindtext}
\usepackage[margin=1in]{geometry}

\usepackage[useregional]{datetime2}
\usepackage{mathtools}

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\title{CS-330 Projet 2020}   % type title between braces
\author{Christophe MARCIOT, Titouan RENARD}         % type author(s) between braces


\begin{document}
\maketitle

\section{Introduction}

\subsection{Goal}

What is intended in this project is to create a tool capable of predicting wheter a patient suffers from a cardiovascular disease given a set of informations.
The informations given for each patients are :
	\begin{itemize}
		\item A variable called \emph{target}, that takes value $0$ or $1$ indicating if the patient does actually suffer from a cardiovascular diesaes or not (here we use the mediacal convention. That is that 0 means negative and 1 means positive in the detection of cardiovascular disease). This is the variable whose behavior we try to predict.
		\item 13 other variables called respectively \emph{sex}, \emph{cp}, \emph{trestbps}, \emph{chol}, \emph{fbs}, \emph{restecg}, \emph{tahlach}, \emph {exang}, \emph{oldpeak}, \emph{slope}, \emph{ca} and \emph{thal} that can take up to $4$ different values.
	\end{itemize}
From now on, we will refer to those $14$  variables as \emph{attributes} and the number taken by those attributes as \emph{values} of the given attribute for the patient. In the end, the tool should also be able to justify its diagnosis, to advise treatements when a disease is diagnosed and be able to treat with continuous data.

\subsection{Structure of the project}
The project is fragmented in $5$ main tasks :
	\begin{itemize} 
		\item Task $1$ : Create a tree capable of guessing the value of \emph{target} for a given patient using a first set of data - given in the file \emph{train\_bin.csv} - using the \emph{ID3} algorithm,
		\item Task $2$ : Test the accuracy of the tree obtained in Task $1$ on a second set of data - given in the file \emph{test\_public\_bin.csv},
		\item Task $3 $: Deduce rules from the tree produced in Task $1$. Design a function capable of predicting the value of \emph{target} using only the aformentionned rules and use them to give a justification of the prediction based on the rules used,
		\item Task $4$ : Improve Task $3$. Design a tool that is capable of the same tasks as in $3$, but it must also be able to give a treatement for the patient when the person is diagnosed with a disease and justify said treatment,
		\item Task $5$ : Improve the \emph{ID3} implementation of Task $1$ such that the new implementation is capable of dealing with continuous data instead of only discrete ones. The construction of the tree uses the data provided in \emph{train\_continuous.csv} and the test of the accuracy uses the data provided in \emph{test\_public\_continuous.csv}.
	\end{itemize}

\section{Discussion of the results}

\subsection{Task $1$}
	The tree genarated for the completion of this task as been genrated by the implementation of the \emph{ID3} algorithm given. We get the following informations about the genrated tree :
	\begin{itemize}
		\item It's composed of $94$ nodes $70$ of wich are leaves,
		\item The size of the tree is 8, which means that the tree wil make at most $8$ desicions before comming to a conclusion,
		\item The average number of children for a node is $3.88$ - without taking leaves into account. This means that at a given time, the tree has on average $4$ methods of classifying a given patient,
		\item The average length of a branch is $3.83$.  This means that, on average, the tree will make $4$ decisions before giving a guess of the value of \emph{target}.
	\end{itemize}
	We can put those numbers into perspective by comparing them to a dichotomy sorting. For $n$ entries in the data set, using dichotomy ,the number of nodes would be $2n-1$ of wich $n$ are leaves, the size of the tree would be $\ceil*{log_2(n)}$, the average number of children for a given node would $2$ and the averange length of a branch would $log_2(n)$. By substituting $n$ for $143$, we get that those numbers are respectively $285$ for the number of nodes, $8$ for the size of the tree and $7.16$ the average length of a branch.\\
	We see that the number of nodes is s lot less than what is expected by dichotomy sorting which is a real improvement in term of space complexity. The average length of a branch is also a lot less which will benefit time complexity. We then conclude that \emph{ID3} benifits both time and space complexity from dichotomy sorting.
		
		
		Include something about the attributes of the node and why it matters.
		
\subsection{Task $2$}
	The test of the tree generated in Task $1$ gives us an accuracy of $56.25\%$ on the testing data. On the training data, this number goes as high as $100\%$. This perfect score on the training data allows us to use the PAC method of statiscal analysis to give a lower bound on the number of exemples needed to obtain a good tree. We use the formula seen in the course and we obtain:
	$$N\geq\frac{log(\delta/|H|)}{log(1-\epsilon)}$$
where $N$ is the number of examples, $\epsilon$ is the error rate we admit for our tree, $\delta$ is the probality of having a tree with error rate $\geq\epsilon$ and $H$ is the set of all possible trees. Here, as we operate in a medical context, we would like to set $\epsilon$  to be low. Let us set $\delta=\epsilon= 0.01$. Now we have to estimate $|H|$. Giving the exact value of $|H|$ is a hard task. The problem lies in the fact that attributes in this problem can take up to 4 values, but for some attributes, such as \emph{sex} and \emph{fbs}, can take less values (only 2 for the mentionned attributes). Here we will rather focus on giving a correct upper and lower bounds while assuming that all attributes can take up to $4$ different values for the upper bound and $2$ for the lower one. This said, we obtain:
	
	$$14*13^2*\ldots*3^{2^{11}}*2^{2^{12}}\approx1.90518*10^{3612}\leq|H|\leq 1.05355*10^{7936038}\approx 14*13^4*\ldots*3^{4^{11}}*2^{4^{12}}  $$
	
	and thus 
	
	$$\frac{log(0.01/1.90518*10^{3612})}{log(0.99)}\approx8.28*10^5\leq\frac{log(0.01/|H|)}{log(0.99)}\leq1.82*10^9\approx\frac{log(0.01/1.05355*10^{7936038})}{log(0.99)}.$$

This allows us to say that that with $N\geq1.82*10^9$ we know that we habe a probability of $0.01$ that the algorithm gives us a tree with an error rate lower than $0.01$ and that $N$ must at least greater than $8.28*10^5$ for it to be the case.

\subsection{Task $3$}
	For this task, we use a \emph{DFS} algorithm on the tree generated in Task $1$ to cover the tree and generate rules based on the nodes covered in th path. Doing this provides on rule per leaf, that is one rule per branch. The rules generated are of the form $$ [(`\mathrm{attribute}_1`,` ?x`, `\mathrm{value}_1`),\ldots,(`\mathrm{attribute}_k`,` ?x`, `\mathrm{value}_k`)]\Rightarrow(`target`, `?x` , `\mathrm{guess}`)$$ where $`?x`$ denotes a variable that will be used in the by the inference engine to do the guess.\\
	To make a deduction from those facts, we use forward chaining while giving to the engine initial facts of the form $[(`age`, `\mathrm{patient\ no}\ k`, `age(\mathrm{patient\ no}\ k)`),\ldots,(`thal`, `\mathrm{patient\ no}\ k`, `thal(\mathrm{patient\ no}\ k)`)]$. We make it so that the engine gives us the guess and the rule that was used to make the deduction. We then use this rule to give the motivation of the guess. Once we get those motivations, we can arrange them in a way to display the diagnosis in a more readable way as can be found in the file \emph{Task3\_data.txt}. Those deductions are made from the test data set.\\
	We notice that one of the guess canno't be done by the inference engine (patient no. $61$). This i sdue to the fact that there are no rules that can treat this case. By guessing using the tree, we have that even if the leaf can not treat the given patient, the predominant class can give a classification nonetheless. That is why, we can produce a guess using the tree while not being able to with the inference engine.
	
\subsection{Task $4$}
	In this part of the project

\subsection{Task $5$}

\section{Conclusion}

\end{document}