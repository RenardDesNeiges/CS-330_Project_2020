
\documentclass[8pt]{extarticle}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{babel}
\usepackage{blindtext}
\usepackage[margin=0.8in]{geometry}

\usepackage[useregional]{datetime2}
\usepackage{mathtools}
\usepackage{enumitem}

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}


\title{CS-330 Project 2020 - Cardiovascular diseases diagnosis}   % type title between braces
\author{Christophe MARCIOT, Titouan RENARD}         % type author(s) between braces


\begin{document}
\maketitle

\section{Introduction}
What is intended in this project is to create a tool capable of predicting whether a patient suffers from a cardiovascular disease given a set of informations.
The informations given for each patients are :
	\begin{itemize}[topsep=0pt,itemsep=0pt,partopsep=0pt, parsep=0pt]
		\item[--] A variable called \emph{target}, that takes value $0$ or $1$ indicating if the patient does actually suffer from a cardiovascular disease or not (0 mean healthy, 1 means sick). This is the variable whose behavior we try to predict.
		\item[--] 13 other variables called respectively \emph{sex}, \emph{cp}, \emph{trestbps}, \emph{chol}, \emph{fbs}, \emph{restecg}, \emph{tahlach}, \emph {exang}, \emph{oldpeak}, \emph{slope}, \emph{ca} and \emph{thal} that can take up to $4$ different values.
	\end{itemize}
From now on, we will refer to those $14$  variables as \emph{attributes} and the number taken by those attributes as \emph{values} of the given attribute for the patient.

\section{Discussion of the results}

\subsection{Task $1$}
	The goal here is to generate a decision tree able to predict the value of \emph{target} for a given patient using a first set of data - given in the file \emph{train\_bin.csv} - this is achieved using the \emph{ID3} algorithm as implemented in the exercises. We get the following informations about the generated tree :
	\begin{itemize}[topsep=0pt,itemsep=0pt,partopsep=0pt, parsep=0pt]
		\item[--] It's composed of $94$ nodes $70$ of which are leaves,
		\item[--] The size of the tree is 8, which means that the tree will make at most $8$ decisions before coming to a conclusion,
		\item[--] The average number of children for a node is $3.88$ - without taking leaves into account.
		\item[--] The average length of a branch is $3.83$.  This means that, on average, the tree will make $4$ decisions before giving a guess of the value of \emph{target}.
	\end{itemize}
	%à voir si on arrive à réintroduire ça mais ça me semble trop long
	%We can put those numbers into perspective by comparing them to a dichotomy sort. For $n$ entries in the data set, using dichotomy ,the number of nodes would be $2n-1$ of which $n$ are leaves, the size of the tree would be $\ceil*{log_2(n)}$, the average number of children for a given node would $2$ and the average length of a branch would $log_2(n)$. By substituting $n$ for $143$, we get that those numbers are respectively $285$ for the number of nodes, $8$ for the size of the tree and $7.16$ the average length of a branch.\\
	%We see that the number of nodes is s lot less than what is expected for dichotomy sort which is a real improvement in term of space complexity. The average length of a branch is also a lot less which will benefit time complexity. We then conclude that \emph{ID3} benefits both time and space complexity from dichotomy sorting.\\
	%CAPARTENANNEXE/20
	% Below, we give a table containing the repartition of attributes along the nodes of the tree
	% \begin{center}
	% 	\begin{tabular}{ |c|c|c|c|c|c|c|c|c|c|c|c|c| }
	% 		\hline
	% 		$age$ & $sex$ & $cp$ & $trestbps$ & $chol$ & $fbs$ & $restecg$ & $thalach$ & $exang$ & $oldpeak$ & $slope$ & $ca$ & $thal$\\
	% 		\hline
	% 		$5$ & $3$ & $1$ & $3$ & $3$ & $0$ & $2$ & $1$ & $1$ & $0$ & $2$  & $2$ & $1$\\
	% 		\hline
	% 	\end{tabular}
	% \end{center}
	We first notice that the $age$ attribute is the most prevalent attribute present in the nodes. This seems actually relevant as one would expect age to be an attribute that minimize the entropy as older people tend to be more suffering from cardiovascular disease. Also we note that the $fbs$ and $oldpeak$ attributes are never used in the tree. This allows to say that, in this particular context, these two variables are useless to the construction of a tree using our \emph{ID3} algorithm.
		
		
\subsection{Task $2$}
	The goal was to evaluate the accuracy of the tree obtained in Task $1$ on a second set of data - given in the file \emph{test\_public\_bin.csv}.
	Our tree has an accuracy of $56.25\%$ on the testing data. This is obviously a disappointing result, we partly attribute the low accuracy of ID3 on disparities between training and testing data, indeed, a quick look at the input datasets shows us that the average value for the \emph{target} attribute in the \emph{train\_bin.csv} file is $0.2307$ whereas it is $0.8250$ for the \emph{test\_public\_bin.csv} dataset, this bias in data selection is likely what throws off the ID3 algorithm. This can be shown by shuffling the data of \emph{train\_bin.csv} and \emph{test\_public\_bin.csv} together and then separating them in new training and testing datasets (of the same sizes as the ones provided), doing this gives on average an accuracy of $69.43\%$, this the way the data is split is "especially bad", we have written an small example program to show this, it is included in the files we are submitting as \emph{shuffled\_data.py}.
	
	On the training data, the accuracy goes as high as $100\%$. Knowing this allows us to use the PAC method of statical analysis to give a lower bound on the number of exemples needed to obtain a good tree. As seen in course we have:
	$N\geq{log(\delta/|H|)}/{log(1-\epsilon)}$
where $N$ is the number of examples, $\epsilon$ is the error rate we admit for our tree, $\delta$ is the probability of having a tree with error rate $\geq\epsilon$ and $H$ is the set of all possible trees. Here, as we operate in a medical context, we would like to set $\epsilon$  to be low. Let $\delta=\epsilon= 0.01$. Now we have to estimate $|H|$. Computing an exact value of $|H|$ is a hard task because some attributes in this problem can take up to 4 values, but for some, such as \emph{sex} and \emph{fbs}, can take less (only 2 for the mentioned attributes). Here we will assume that all attributes can take up to $4$ different values for the upper bound and $2$ for the lower one. We get: $8.28*10^5\leq\frac{log(0.01/|H|)}{log(0.99)}\leq1.82*10^9.$ \\
	
	%again c'est la guerre pour gratter des lignes
	%$$14*13^2*\ldots*3^{2^{11}}*2^{2^{12}}\approx1.90518*10^{3612}\leq|H|\leq 1.05355*10^{7936038}\approx 14*13^4*\ldots*3^{4^{11}}*2^{4^{12}}  $$
	
	%and thus 
	
	%$$\frac{log(0.01/1.90518*10^{3612})}{log(0.99)}\approx8.28*10^5\leq\frac{log(0.01/|H|)}{log(0.99)}\leq1.82*10^9\approx\frac{log(0.01/1.05355*10^{7936038})}{log(0.99)}.$$

	

This allows us to say that that with $N\geq1.82*10^9$ we know that we have a probability of $0.01$ that the algorithm gives us a tree with an error rate lower than $0.01$ and that $N$ must at least greater than $8.28*10^5$ for it to be the case.

\subsection{Task $3$}
	We want to predict the value of \emph{target} using only the aforementioned rules and use them to give a justification of the prediction.
	We use a \emph{DFS} algorithm on the tree generated in Task $1$ and generate rules based on the nodes covered in its path. Doing this provides on rule per leaf, that is one rule per path. The rules generated are of the form $ [(`\mathrm{attribute}_1`,` ?x`, `\mathrm{value}_1`),\ldots,(`\mathrm{attribute}_k`,` ?x`, `\mathrm{value}_k`)]\Rightarrow(`target`, `?x` , `\mathrm{guess}`)$ where $`?x`$ denotes a variable.\\
	To make a deduction from those facts, we use forward chaining while giving to the engine initial facts of the form $[(`age`, `\mathrm{patient\ no}\ k`, `age(\mathrm{patient\ no}\ k)`),\ldots,(`thal`, `\mathrm{patient\ no}\ k`, `thal(\mathrm{patient\ no}\ k)`)]$. We format the deductions of the inference engine to display the diagnosis in a more readable way as can be found in the file \emph{Task3\_data.txt}. Those deductions are made from the test data set.\\
	We notice that one of the guesses cannot be done by the inference engine (patient no. $61$). This is due to the fact that there are no rules that can explain this case. The \emph{classifie} method of the \emph{NoeudDeDecision} class classifies it by simply affecting it to the predominant class. This did not seem to us to be a useful justification, therefore we chose to produce no deductions using the inference engine.
	
\subsection{Task $4$}
	In this part, we implement a tool capable of prescribing a treatment when necessary. We assume that a treatment is necessary for a given patient if the diagnosis proposed in Task $3$ concludes that $target=1$ \emph{or} if the procedure in Task $3$ could not draw any conclusion.\\
	We produce a treatment by modeling this as a problem of diagnosis by induction. Here, we assume that each value to each attribute can be considered as defective. For a given patient, we use a \emph{BFS} algorithm on the following tree:
	\begin{itemize}[topsep=0pt,itemsep=0pt,partopsep=0pt, parsep=0pt]
		\item[--] To each node is associated a \emph{change}. A change is a list of attributes and values such that the value associated to the given attribute is different from the value associated to the same attribute and the patient.
		\item[--] The initial node is associated to the imply change.
		\item[--] The children of a given node are the changes that are just the change of the current node to which we add an attribute that is not already present in it with a value that is different to the value associated to the given attribute and the patient.
		\item[--] When coming to a node, we diagnose the patient whose information are updated with the values present in the change. If the diagnosis returns $target=0$, we then consider that the change is the treatment. Also we use the rule used in the diagnosis to justify the treatment. If the diagnosis returns $target=1$ we then queue up the children of the current node in the queue - as it is usually done in \emph{BFS}.
	\end{itemize}
Restricting us to changes of size smaller or equal to $2$, we see that we can actually treat all patients that were diagnosed to be ill. In fact we notice that there usually more than $400$ changes available of size smaller or equal to $2$ and thus it is not really surprising to see a treatment to be possible. Also we have designed a function that is able to represent the diagnosis and the treatment as well as the justifications for both of them. A list of all diagnosis' and treatments cases be found in the file \emph{Task4\_data.txt}.

\subsection{Task $5$}
The goal was to improve the \emph{ID3} implementation of Task $1$ to make it able to deal with continuous attribute values instead of only discretized ones.
In this part of the project, we modify the ID3 algorithm so that it is able to deal we continuous instead of binned values data attributes. Unlike the previous implementation, the new algorithm produces binary trees. At each node, patients as separated in two subsets according to a threshold value on the attribute that maximizes : $[H(C) - H(C|A)] / {values(A)}$. This threshold value is selected to minimize the entropy of the target value after partitioning. \\
Because of the small size of the dataset, we did not implement a root-finding algorithm, we simply try every value in between two existing values in the dataset for the attribute we are evaluating. That being said, such an algorithm would definitely be worth implementing if we were to consider significantly larger datasets. \\
The performance of the continuous diagnosis algorithm is slightly but not significantly better than discrete one. We get an accuracy of $58.75\%$. As for task 2 we think that this is partly due to the way the data-points are separated between the \emph{test\_public\_continuous.csv} and \emph{train\_continuous.csv}. 

\subsection{Task $2$ bis}
	In this supplementary section, we tried to implement the \emph{random forests} algorithm. We generate trees with randomly chosen subparts of the original training data set and then consider only the ones that can treat the whole original training data set. While testing the trees, we also compute their accuracy on the original data set.Then we can use $3$ different classifiers. Here we implemented the \emph{BESTTREE}, the \emph{MAJORITYVOTE} and the \emph{ADABOOST} algorithms. We describe them the following way :
	\begin{itemize}[topsep=0pt,itemsep=0pt,partopsep=0pt, parsep=0pt]
		\item[--] \emph{BESTTREE} algorithm : Choose the tree with the greater accuracy on the training data set and use it as a classifier,
		\item[--] \emph{MAJORITYVOTE} algorithm : Consider the guesses of all the trees generated and then gives a guess corresponding to the majority of the guesses,
		\item[--] \emph{ADABOOST} algorithm : Consider only the trees with a probability of error smaller to $50\%$  - as weak methods - and construct a classifier using the \emph{ADABOOST} algorithm presented in class.
	\end{itemize}
	We tested the accuracy of these algorithms on the test data set by genreating $1000$ trees from a randomly chosen subsets of the training data set. The size of the subset is detemined by the value of \emph{subsampling}. For a value $n$ of subsampling, we get that the subsets are of size $\floor{143/n}$ As the randomness play a big role in, we computed the accuracy $100$ times and considered the mean of these accuracies. The means of the computed accuracies are given in the following table.
	\begin{center}
		\begin{tabular}{ |c|c|c|c| }
			\hline
			subsampling/algorithm& \emph{BESTTREE} & \emph{MAJORITYVOTE} & \emph{ADABOOST}\\
			\hline
			2 & $\approx 0.53$  & $\approx 0.50$ & $\approx 0.53$ \\
			\hline
			3 & $\approx 0.51 $ & $\approx 0.47$ & $\approx 0.53$ \\
			\hline
			4 & $\approx 0.48$ & $\approx 0.42$ & $\approx 0.55$\\
			\hline
		\end{tabular}
	\end{center}
	We clearly see that the \emph{BESTTREE} and \emph{MAJORITYVOTE} algorithms perform as good as or worse than the expected accuracy of guessing randomly which allows us to conclude that they are practically useless when the subsampling is greater than $2$.  We also notice that while both \emph{BESTTREE} and \emph{MAJORITYVOTE} perfom worse when the subsampling gets greater, the performance of \emph{ADABOOST} does not change. As the trees are generated from smaller training sets every row, it seems that on average, those trees get less effective in their guesses. The impact is visible on \emph{BESTTREE} and \emph{MAJORITYVOTE}. On \emph{ADABOOST} we see that despite the augmentation of the subsampling value, the accuracy stays the same. We conclude that the \emph{ADABOOST}algorithm is in fact the better one. This said we also have to observe that generating a tree with \emph{ID3} is still the better solution.

\end{document}